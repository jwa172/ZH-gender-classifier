{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3098a0ac",
   "metadata": {},
   "source": [
    "## Mandarin name gender predictor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f19ec58",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Binary classification\n",
    "1. \n",
    "\n",
    "- Using Bag of words\n",
    "- Using Pinyin (tone on/off is a parameter)\n",
    "- Using Chinese word embeddings (expect a much higher accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "caf229f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_data():\n",
    "    df = pd.read_excel(\"./Data/9800ChineseNamesnamegender_ORIGINAL.xlsx\")\n",
    "    df = df[[\"姓名\", \"性别\"]]\n",
    "    df.columns =[\"Name\", \"Gender\"]\n",
    "\n",
    "    # Remove surname\n",
    "    df['Name'] = df['Name'].map(lambda x: x[1:])\n",
    "\n",
    "    # Remove 3-character names\n",
    "    nameLength = df['Name'].map(len)\n",
    "    df = df[nameLength.between(1,2)]\n",
    "\n",
    "    # Encode the labels => M:0, F:1\n",
    "    df[\"Gender\"] = df[\"Gender\"].map({\"男\":0, \"女\":1})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd15f909",
   "metadata": {},
   "source": [
    "### 1. Inputs containing only phonetic information (pinyin only)\n",
    "\n",
    "This is applicable to the original question at hand. However, since we are completely discarding the semantic information of the character, it not expected to perform as well as using/combining character embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "58cd7d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chao</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fang zhou</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lin feng</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Name  Gender\n",
       "0       chao       0\n",
       "1  fang zhou       0\n",
       "2   lin feng       0"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pinyin\n",
    "\n",
    "def getPinyinDF():\n",
    "\n",
    "    def processToPinyin(x):\n",
    "        # Strip removes tones\n",
    "        return pinyin.get(x, format=\"strip\", delimiter=\" \")\n",
    "\n",
    "    df = load_data()\n",
    "    df[\"Name\"] = df[\"Name\"].map(processToPinyin)\n",
    "    return df\n",
    "\n",
    "df = getPinyinDF()\n",
    "df.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e192541e",
   "metadata": {},
   "source": [
    "##### 1.1. Bag of word representation\n",
    "\n",
    "Convert each pinyin syllable to the BoW representation. This approach works suprisingly well because there's only around 400 valid syllables in Mandarin (1600 if we also include the tone combinations).\n",
    "\n",
    "<img src=\"img/BoW.png\" width=450 height=250/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bfa6cc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBagOfWords(names):\n",
    "\n",
    "    # 'chao' -> ['chao'], 'fang zhou' -> ['fang', 'zhou']\n",
    "    names = names.map(lambda x: x.split()).to_list()\n",
    "    \n",
    "    # Flatten the list of lists and get all the unique values\n",
    "    uniqueNames = set(i for j in names for i in j)\n",
    "    mappingDict = {n: i for i, n in enumerate(uniqueNames)}\n",
    "\n",
    "    # R : number of training instances\n",
    "    R, N = len(df), len(mappingDict)\n",
    "    bagOfWords = np.zeros((R, N))\n",
    "\n",
    "    for key, val in enumerate(names):\n",
    "        for i in val:\n",
    "            bagOfWords[key][mappingDict[i]] += 1\n",
    "\n",
    "    return bagOfWords"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8895a2f4",
   "metadata": {},
   "source": [
    "The performance of baseline models on BoW without any hyperparameter tuning is reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "370a1997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\j_ahn\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of KNeighborsClassifier: 0.6553\n",
      "Accuracy of DecisionTreeClassifier: 0.6865\n",
      "Accuracy of RandomForestClassifier: 0.6923\n",
      "Accuracy of BernoulliNB: 0.7005\n",
      "Accuracy of MultinomialNB: 0.7009\n",
      "Accuracy of LogisticRegression: 0.7046\n",
      "Accuracy of SVC: 0.7120\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def evalBaseModels(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "    # Initialise all models (no optimization performed)\n",
    "    models = [KNeighborsClassifier(),\n",
    "              BernoulliNB(),\n",
    "              MultinomialNB(),\n",
    "              LogisticRegression(random_state=0),\n",
    "              DecisionTreeClassifier(random_state=0),\n",
    "              SVC(random_state=0),\n",
    "              RandomForestClassifier(random_state=0)]\n",
    "\n",
    "    modelAcc = {}\n",
    "    for clf in models:\n",
    "        try:\n",
    "            clf.fit(X_train, y_train)\n",
    "            modelAcc[clf.__class__.__name__] = clf.score(X_test, y_test)\n",
    "        except:\n",
    "            Exception\n",
    "\n",
    "    # Print according to ascending accuracy\n",
    "    accs = sorted(modelAcc.items(), key=lambda x:x[1])\n",
    "    for model, acc in accs:\n",
    "        print(f\"Accuracy of {model}: {acc:.4f}\")\n",
    "\n",
    "evalBaseModels(getBagOfWords(df[\"Name\"]), df[\"Gender\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0fdd2a92",
   "metadata": {},
   "source": [
    "#### 1.2. Bi-LSTM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7c684ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "df = getPinyinDF()\n",
    "\n",
    "# 1. Get the (unique) character list and mapping dictionary\n",
    "def getCList(names):\n",
    "    charList = list(string.ascii_lowercase)\n",
    "    charList += ['[PAD]', '[UNKNOWN]']\n",
    "    charList.sort()\n",
    "\n",
    "    # 0 -> [1, 0, ...], 1 -> [0, 1, 0, ...]\n",
    "    def getOneHotVec(i):\n",
    "        x = np.zeros(len(charList)); x[i] = 1\n",
    "        return x\n",
    "\n",
    "    oneHotMapper = {val: getOneHotVec(i) for i, val in enumerate(charList)}\n",
    "    return (charList, oneHotMapper)\n",
    "\n",
    "def getEncodedNames(names, mappingDict):\n",
    "    seq_length = max(len(n) for n in names)\n",
    "    namesEncoded = []\n",
    "    for n in names:\n",
    "        name = []\n",
    "        for i in range(seq_length):\n",
    "            # Pad characters if reached the end or space of 2 characters\n",
    "            if i > len(n)-1 or n[i]==' ':\n",
    "                name.append(mappingDict['[PAD]'])\n",
    "            # Unknown character\n",
    "            elif n[i] not in mappingDict:\n",
    "                name.append(mappingDict['[UNKNOWN]'])\n",
    "            else:\n",
    "                name.append(mappingDict[n[i]])\n",
    "        namesEncoded.append(np.array(name))\n",
    "    return np.array(namesEncoded)\n",
    "\n",
    "names = df['Name']\n",
    "charList, mapper = getCList(names)\n",
    "encodedNames = getEncodedNames(names, mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "4448c853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, loss: 0.5921, train accuracy: 0.6853\n",
      "Epoch 100, loss: 0.5373, train accuracy: 0.7206\n",
      "Epoch 150, loss: 0.4816, train accuracy: 0.7460\n",
      "Epoch 200, loss: 0.4478, train accuracy: 0.7699\n",
      "Epoch 250, loss: 0.4203, train accuracy: 0.7876\n",
      "Epoch 300, loss: 0.4171, train accuracy: 0.7912\n",
      "Epoch 350, loss: 0.3806, train accuracy: 0.8122\n",
      "Epoch 400, loss: 0.3587, train accuracy: 0.8270\n",
      "Epoch 450, loss: 0.3590, train accuracy: 0.8231\n",
      "Epoch 500, loss: 0.3571, train accuracy: 0.8251\n",
      "\n",
      "Test accuracy : 0.6689\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class Bi_LSTM1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Bi_LSTM1, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.linear = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get the embeded tensor\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n",
    "        z = self.linear(hidden_out)\n",
    "        return z\n",
    "\n",
    "def trainNetwork(X, y, model, criterion, optimiser, nEpochs=1000, printInterval=200):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "    for epoch in range(nEpochs):\n",
    "\n",
    "        # Train mode\n",
    "        model.train()\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # Forward-propagation\n",
    "        y_pred = model(X_train)\n",
    "\n",
    "        # Calculate error\n",
    "        loss = criterion(y_pred, y_train)\n",
    "\n",
    "        # Optimise\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        # Print statistics (every 200 epochs)\n",
    "        if epoch % printInterval == printInterval-1:    \n",
    "            model.eval()\n",
    "            predicted = torch.argmax(y_pred, 1)\n",
    "            train_acc = accuracy_score(predicted, y_train)\n",
    "            print('Epoch %d, loss: %.4f, train accuracy: %.4f' %(epoch + 1, loss.item(), train_acc))\n",
    "\n",
    "    # Result\n",
    "    y_pred = model(X_test)\n",
    "    predicted = torch.argmax(y_pred, 1)\n",
    "    testAcc = accuracy_score(predicted, y_test)\n",
    "    print(f'\\nTest accuracy : {testAcc:.4f}')\n",
    "\n",
    "hidden_size = 10\n",
    "model = Bi_LSTM1(len(charList), hidden_size, 2)\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "X = torch.from_numpy(encodedNames).float()\n",
    "y = torch.from_numpy(np.array(df['Gender']))\n",
    "\n",
    "trainNetwork(X, y, model, criterion, optimizer, 500, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for xiao long: Male\n",
      "Prediction for xiang dong: Male\n",
      "Prediction for yi nuo: Female\n",
      "Prediction for xiao ming: Female\n",
      "Prediction for xin yi: Female\n",
      "Prediction for zi han: Male\n"
     ]
    }
   ],
   "source": [
    "def getPrediction(model, name, mappingDict):\n",
    "    seq_length = len(charList)\n",
    "    nameEncoding = []\n",
    "    for i in range(seq_length):\n",
    "        if i > len(name)-1 or name[i]==' ':\n",
    "            nameEncoding.append(mappingDict['[PAD]'])\n",
    "        # Unknown character\n",
    "        elif name[i] not in mappingDict:\n",
    "            nameEncoding.append(mappingDict['[UNKNOWN]'])\n",
    "        else:\n",
    "            nameEncoding.append(mappingDict[name[i]])\n",
    "    nameEncoding = np.array([np.array(nameEncoding)])\n",
    "    output = model(torch.Tensor(nameEncoding))\n",
    "    y_pred = int(torch.argmax(output))\n",
    "    D = {0:\"Male\", 1:\"Female\"}\n",
    "    print(f\"Prediction for {name}: {D[y_pred]}\")\n",
    "\n",
    "names = ['xiao long',\n",
    "         'xiang dong',\n",
    "         'yi nuo',\n",
    "         'xiao ming',\n",
    "         'xin yi',\n",
    "         'zi han']\n",
    "\n",
    "for name in names: getPrediction(model, name, mapper)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c1fba4e",
   "metadata": {},
   "source": [
    "### 2. Inputs containing all character information (pre-trained embeddings)\n",
    "\n",
    "Directly use Chinese characters via embeddings. The embeddings can be trained separately but we use pre-trained embeddings derived here : https://www.kaggle.com/datasets/guiyihan/chinesewordvectors.\n",
    "The embedding dimension is 300 (assumed to be be Word2Vec embeddings).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "df59b6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# May take a few minutes\n",
    "embeddings = {}\n",
    "with open('./Data/sgns.merge.word.txt', 'r') as f:\n",
    "    # Skip first line\n",
    "    next(f)\n",
    "    for line in f:\n",
    "        # Split once\n",
    "        char, vec = line.split(' ', 1)\n",
    "        embeddings[char] = np.fromstring(vec, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "aa619966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embedding of each name\n",
    "def getNameEmbeddings(nameList, embDict):\n",
    "\n",
    "    emb = []\n",
    "    for name in nameList:\n",
    "\n",
    "        # 1. Get embedding of each character\n",
    "        tmp = [embDict.get(char, np.zeros(300)) for char in name]\n",
    "\n",
    "        # 2. Get the average of the embeddings\n",
    "        # Averaging is the most common way but array concatenation could be also\n",
    "        # be considered especially since we only have a maximum of 2 characters\n",
    "        tmp = np.mean(tmp, axis=0)\n",
    "\n",
    "        # 3. Add to list\n",
    "        emb.append(tmp)\n",
    "\n",
    "    return np.array(emb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4cb71d8c",
   "metadata": {},
   "source": [
    "The performance of baseline models on character embeddings is noticably better than the BoW representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "78eec4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\j_ahn\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of DecisionTreeClassifier: 0.7071\n",
      "Accuracy of BernoulliNB: 0.7301\n",
      "Accuracy of KNeighborsClassifier: 0.7486\n",
      "Accuracy of RandomForestClassifier: 0.7675\n",
      "Accuracy of LogisticRegression: 0.7818\n",
      "Accuracy of SVC: 0.7925\n"
     ]
    }
   ],
   "source": [
    "df = load_data()\n",
    "X, y = getNameEmbeddings(df['Name'], embeddings), np.array(df['Gender'])\n",
    "evalBaseModels(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a0eaef8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200, loss: 0.6725, train accuracy: 0.5144\n",
      "Epoch 400, loss: 0.6288, train accuracy: 0.7488\n",
      "Epoch 600, loss: 0.5800, train accuracy: 0.7703\n",
      "Epoch 800, loss: 0.5354, train accuracy: 0.7796\n",
      "Epoch 1000, loss: 0.5023, train accuracy: 0.7876\n",
      "\n",
      "Test accuracy : 0.7576\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ANN1(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ANN1, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z1 = self.linear1(x)\n",
    "        Zout = self.linear2(F.relu(z1))\n",
    "        return Zout\n",
    "\n",
    "model = ANN1(300, 10, 2)\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimiser = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "X = torch.from_numpy(getNameEmbeddings(df['Name'], embeddings)).float()\n",
    "y = torch.from_numpy(np.array(df['Gender']))\n",
    "\n",
    "trainNetwork(X, y, model, criterion, optimiser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d8539fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for 马云: Female\n",
      "Prediction for 赵丽颖: Female\n",
      "Prediction for 周杰伦: Male\n",
      "Prediction for 邓小平: Male\n"
     ]
    }
   ],
   "source": [
    "def getPrediction(model, name, embDict):\n",
    "    nameEmb = np.mean([embDict.get(char, np.zeros(300)) for char in name[1:]], axis=0)\n",
    "    output = model(torch.Tensor(nameEmb))\n",
    "    y_pred = int(torch.argmax(output))\n",
    "    D = {0:\"Male\", 1:\"Female\"}\n",
    "    print(f\"Prediction for {name}: {D[y_pred]}\")\n",
    "\n",
    "names = ['马云',    # Jack Ma\n",
    "         '赵丽颖',  # Zanilia Zhao\n",
    "         '周杰伦',   # Jay chou\n",
    "         '邓小平']   # Deng xiaoping\n",
    "\n",
    "for name in names: getPrediction(model, name, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "20d07c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get the (unique) character list\n",
    "char_set = set(list(\"\".join(df['Name'])))\n",
    "char_set.add('[PAD]'); char_set.add('[UNKNOWN]')\n",
    "char_list = list(char_set)\n",
    "char_list.sort()\n",
    "\n",
    "# 2. Make character index\n",
    "char_index = {val: index for index, val in enumerate(char_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "09d97ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEmbeddingTable(embDict, char_list):\n",
    "    emb_table = []\n",
    "    for c in char_list:\n",
    "        emb = embDict[c] if c in embeddings else [0]*300\n",
    "        emb_table.append(emb)\n",
    "    return torch.from_numpy(np.array(emb_table))\n",
    "\n",
    "def getEncodedNames(char_index):\n",
    "    names_encoded = []\n",
    "    for name in df['Name']:\n",
    "        tmp = []\n",
    "        if len(name) == 1:\n",
    "            tmp = [char_index[name]] + [char_index['[PAD]']]\n",
    "        else:\n",
    "            for char in name:\n",
    "                tmp += [char_index[char]]\n",
    "        names_encoded.append(tmp)\n",
    "    return torch.from_numpy(np.array(names_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "56a8b6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_table = getEmbeddingTable(embeddings, char_list)\n",
    "char_count = len(char_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "45c080c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, loss: 0.4162, train accuracy: 0.8062\n",
      "Epoch 40, loss: 0.3395, train accuracy: 0.8489\n",
      "Epoch 60, loss: 0.2613, train accuracy: 0.8903\n",
      "Epoch 80, loss: 0.1915, train accuracy: 0.9234\n",
      "Epoch 100, loss: 0.1446, train accuracy: 0.9408\n",
      "Epoch 120, loss: 0.1212, train accuracy: 0.9462\n",
      "Epoch 140, loss: 0.1110, train accuracy: 0.9467\n",
      "Epoch 160, loss: 0.1064, train accuracy: 0.9467\n",
      "Epoch 180, loss: 0.1046, train accuracy: 0.9466\n",
      "Epoch 200, loss: 0.1028, train accuracy: 0.9467\n",
      "\n",
      "Test accuracy : 0.7728\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the model\n",
    "class Bi_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Bi_LSTM, self).__init__()\n",
    "\n",
    "        self.emb = nn.Embedding(char_count, input_size)\n",
    "        self.emb.weight.data.copy_(emb_table)\n",
    "        self.emb.weight.requires_grad = False\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.linear = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get the embedded tensor\n",
    "        x = self.emb(x)        \n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n",
    "        z = self.linear(hidden_out)\n",
    "        return z\n",
    "\n",
    "hidden_size = 10\n",
    "model = Bi_LSTM(300, hidden_size, 2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "X_train = getEncodedNames(char_index)\n",
    "\n",
    "X = getEncodedNames(char_index)\n",
    "y = torch.from_numpy(np.array(df['Gender']))\n",
    "\n",
    "trainNetwork(X, y, model, criterion, optimizer, nEpochs=200, printInterval=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
